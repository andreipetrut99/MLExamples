{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "train_text = [\n",
    "    'This is an example of how to use Count Vectorizer.',\n",
    "    'We will learn how to generate n-grams count with Count Vectorizer',\n",
    "    'Also, this example will teach you how to filter stop words with Count Vectorizer',\n",
    "]\n",
    "\n",
    "test_text = [\n",
    "    'Test example with unknown tokens',\n",
    "    'Count Vectorizer is the best way to generate n-gram counts'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[1 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0 1 0 0 1 1 1 1 1 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 1 1 1 1 0 0 2 1 0 1 0 0 0 0 1 1 0 0\n",
      "  1 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 2 2 0 0 1 1 0 0 0 1 1 0\n",
      "  0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 0 0 1 1 1 0 0 1 0 0 1 1 0 0 1 0 1 1 0 1 1\n",
      "  1 0 0 1 1 0 0 1 0 0 0 0 0 1 1 0 0 2 1 0 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0\n",
      "  1 0 1 2 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 1 1 0 0 1 1]\n",
      " [0 0 2 2 0 0 0 0 1 1 1 1 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1 2 2 0 0 0 0\n",
      "  0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 1 0 0 2 2 1 1 0 0 2 0 1 0 1 1 0 1 1 1 1 1\n",
      "  2 0 0 1 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 0 1\n",
      "  1 1 0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 2 2 1 0 1 0 0 0 0 0 0 0 1 0 1 2 0 2 1\n",
      "  1 0 0 0 0 0 0 0 2 1 1 0 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 2 1 1 1 1 0 0\n",
      "  1 1 0 2 1 0 1 0 0 2 2 0 0 1 1 1 1 1 1 2 1 1 0 0 0 0 0 0 1 1]\n",
      " [0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 1 1 3 1 1 1 0 0 1 1 3 2 1 1 1 1\n",
      "  1 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 0 1 1 0 0\n",
      "  2 1 0 0 1 1 0 0 1 1 0 0 0 0 2 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 0 1 1 1 0\n",
      "  1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 1 1 0 0 1 1 2 1 1 2 1 1 1\n",
      "  1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0 2 0 0 1 0 1 0 0 1 1 1 1 1 1 0 2 0 1 1\n",
      "  2 1 1 3 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0 2 1 1 1 1 1 1 1 1 1 1]]\n",
      "{'th': 180, 'hi': 89, 'is': 96, 's ': 161, ' i': 12, ' a': 0, 'an': 46, 'n ': 118, ' e': 4, 'ex': 76, 'xa': 204, 'am': 43, 'mp': 114, 'pl': 147, 'le': 105, 'e ': 60, ' o': 18, 'of': 133, 'f ': 78, ' h': 10, 'ho': 91, 'ow': 143, 'w ': 195, ' t': 22, 'to': 183, 'o ': 127, ' u': 26, 'us': 191, 'se': 167, ' c': 2, 'co': 54, 'ou': 140, 'un': 189, 'nt': 125, 't ': 173, ' v': 28, 've': 193, 'ec': 68, 'ct': 56, 'or': 137, 'ri': 157, 'iz': 100, 'ze': 208, 'er': 72, 'r.': 151, 'thi': 182, 'his': 90, 'is ': 97, 's i': 165, ' is': 13, 's a': 162, ' an': 1, 'an ': 47, 'n e': 119, ' ex': 5, 'exa': 77, 'xam': 205, 'amp': 44, 'mpl': 115, 'ple': 148, 'le ': 106, 'e o': 63, ' of': 19, 'of ': 134, 'f h': 79, ' ho': 11, 'how': 92, 'ow ': 144, 'w t': 196, ' to': 25, 'to ': 184, 'o u': 130, ' us': 27, 'use': 192, 'se ': 168, 'e c': 61, ' co': 3, 'cou': 55, 'oun': 142, 'unt': 190, 'nt ': 126, 't v': 174, ' ve': 29, 'vec': 194, 'ect': 69, 'cto': 57, 'tor': 186, 'ori': 139, 'riz': 158, 'ize': 101, 'zer': 209, 'er.': 74, 'we': 197, ' w': 30, 'wi': 199, 'il': 93, 'll': 108, 'l ': 102, ' l': 14, 'ea': 65, 'ar': 48, 'rn': 159, ' g': 8, 'ge': 82, 'en': 70, 'ne': 123, 'ra': 152, 'at': 50, 'te': 176, ' n': 16, 'n-': 121, '-g': 37, 'gr': 84, 'ms': 116, 'it': 98, 'h ': 86, 'we ': 198, 'e w': 64, ' wi': 31, 'wil': 200, 'ill': 94, 'll ': 109, 'l l': 103, ' le': 15, 'lea': 107, 'ear': 67, 'arn': 49, 'rn ': 160, 'n h': 120, 'o g': 129, ' ge': 9, 'gen': 83, 'ene': 71, 'ner': 124, 'era': 75, 'rat': 154, 'ate': 51, 'te ': 177, 'e n': 62, ' n-': 17, 'n-g': 122, '-gr': 38, 'gra': 85, 'ram': 153, 'ams': 45, 'ms ': 117, 's c': 163, 't w': 175, 'wit': 201, 'ith': 99, 'th ': 181, 'h c': 87, 'al': 41, 'ls': 110, 'so': 169, 'o,': 131, ', ': 35, 'ac': 39, 'ch': 52, ' y': 33, 'yo': 206, 'u ': 187, ' f': 6, 'fi': 80, 'lt': 112, 'r ': 149, ' s': 20, 'st': 171, 'op': 135, 'p ': 145, 'wo': 202, 'rd': 155, 'ds': 58, 'als': 42, 'lso': 111, 'so,': 170, 'o, ': 132, ', t': 36, ' th': 24, 's e': 164, 'l t': 104, ' te': 23, 'tea': 178, 'eac': 66, 'ach': 40, 'ch ': 53, 'h y': 88, ' yo': 34, 'you': 207, 'ou ': 141, 'u h': 188, 'o f': 128, ' fi': 7, 'fil': 81, 'ilt': 95, 'lte': 113, 'ter': 179, 'er ': 73, 'r s': 150, ' st': 21, 'sto': 172, 'top': 185, 'op ': 136, 'p w': 146, ' wo': 32, 'wor': 203, 'ord': 138, 'rds': 156, 'ds ': 59, 's w': 166}\n",
      "[[0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 1 0 0 0 0\n",
      "  0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0\n",
      "  0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0\n",
      "  0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      "  0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0\n",
      "  1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 0 0 0 0]\n",
      " [0 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 2 0 1 1 0 0 1 1 1 0 0 0 0 0\n",
      "  0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 2 2 1 1 0 0 2 0 1 0 0 0 0 0 1 1 1 1\n",
      "  2 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 2 1 1 0 1 0 0 0 0 0 0 0 1 0 1 2 0 2 0\n",
      "  0 0 0 0 0 1 0 0 2 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 2 1 1 1 1 0 0\n",
      "  1 0 0 2 1 0 1 0 0 2 2 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "count_vec = CountVectorizer(lowercase=True, analyzer='char', ngram_range=(2, 2))\n",
    "#count_vec = CountVectorizer(lowercase=True, analyzer='word', stop_words='english')\n",
    "\n",
    "#count_vec.fit(train_text) # Etapa de antrenare - invatarea vocabular\n",
    "feature_matrix = count_vec.fit_transform(train_text) # Generare de feature-uri cu modelul antrenat\n",
    "\n",
    "#feature_matrix = count_vec.fit_transform(train_text) # Antrenare si generare de feature-uri in acelasi pas\n",
    "\n",
    "print(type(feature_matrix))\n",
    "print(feature_matrix.toarray())\n",
    "\n",
    "print(count_vec.vocabulary_)\n",
    "\n",
    "test_feature_matrix = count_vec.transform(test_text)\n",
    "print(test_feature_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
