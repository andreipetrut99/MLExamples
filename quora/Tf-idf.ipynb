{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import math\n",
    "\n",
    "docs=['I really like to play football and watch football on TV. Football is my favourite sport',\n",
    "      'My favourite TV serial is really awesome and I enjoy watching it',\n",
    "      'I dont really want to talk about my favourite colour',\n",
    "      'My favourite subject in school is computer science, because computer science is awesome.'\n",
    "      \n",
    "     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sent_id': 0,\n",
       "  'sent_length': 17,\n",
       "  'freq_dict': {'i': 1,\n",
       "   'really': 1,\n",
       "   'like': 1,\n",
       "   'to': 1,\n",
       "   'play': 1,\n",
       "   'football': 3,\n",
       "   'and': 1,\n",
       "   'watch': 1,\n",
       "   'on': 1,\n",
       "   'tv': 1,\n",
       "   '.': 1,\n",
       "   'is': 1,\n",
       "   'my': 1,\n",
       "   'favourite': 1,\n",
       "   'sport': 1}},\n",
       " {'sent_id': 1,\n",
       "  'sent_length': 12,\n",
       "  'freq_dict': {'my': 1,\n",
       "   'favourite': 1,\n",
       "   'tv': 1,\n",
       "   'serial': 1,\n",
       "   'is': 1,\n",
       "   'really': 1,\n",
       "   'awesome': 1,\n",
       "   'and': 1,\n",
       "   'i': 1,\n",
       "   'enjoy': 1,\n",
       "   'watching': 1,\n",
       "   'it': 1}},\n",
       " {'sent_id': 2,\n",
       "  'sent_length': 10,\n",
       "  'freq_dict': {'i': 1,\n",
       "   'dont': 1,\n",
       "   'really': 1,\n",
       "   'want': 1,\n",
       "   'to': 1,\n",
       "   'talk': 1,\n",
       "   'about': 1,\n",
       "   'my': 1,\n",
       "   'favourite': 1,\n",
       "   'colour': 1}},\n",
       " {'sent_id': 3,\n",
       "  'sent_length': 15,\n",
       "  'freq_dict': {'my': 1,\n",
       "   'favourite': 1,\n",
       "   'subject': 1,\n",
       "   'in': 1,\n",
       "   'school': 1,\n",
       "   'is': 2,\n",
       "   'computer': 2,\n",
       "   'science': 2,\n",
       "   ',': 1,\n",
       "   'because': 1,\n",
       "   'awesome': 1,\n",
       "   '.': 1}}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_document_freq_dict(sents):\n",
    "    freq_dicts = []\n",
    "    for sent_index, sent in enumerate(sents):\n",
    "        freq_dict = {}\n",
    "        tokens = word_tokenize(sent)\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "        \n",
    "        for token in tokens:\n",
    "            if freq_dict.get(token):\n",
    "                freq_dict[token] += 1\n",
    "            else:\n",
    "                freq_dict[token] = 1\n",
    "                \n",
    "            data_collector = {'sent_id': sent_index, 'sent_length': len(tokens), 'freq_dict': freq_dict}\n",
    "        freq_dicts.append(data_collector)\n",
    "        \n",
    "    return freq_dicts\n",
    "\n",
    "get_document_freq_dict(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sent_id': 0, 'tf_score': 0.058823529411764705, 'token': 'i'},\n",
       " {'sent_id': 0, 'tf_score': 0.058823529411764705, 'token': 'really'},\n",
       " {'sent_id': 0, 'tf_score': 0.058823529411764705, 'token': 'like'},\n",
       " {'sent_id': 0, 'tf_score': 0.058823529411764705, 'token': 'to'},\n",
       " {'sent_id': 0, 'tf_score': 0.058823529411764705, 'token': 'play'},\n",
       " {'sent_id': 0, 'tf_score': 0.17647058823529413, 'token': 'football'},\n",
       " {'sent_id': 0, 'tf_score': 0.058823529411764705, 'token': 'and'},\n",
       " {'sent_id': 0, 'tf_score': 0.058823529411764705, 'token': 'watch'},\n",
       " {'sent_id': 0, 'tf_score': 0.058823529411764705, 'token': 'on'},\n",
       " {'sent_id': 0, 'tf_score': 0.058823529411764705, 'token': 'tv'},\n",
       " {'sent_id': 0, 'tf_score': 0.058823529411764705, 'token': '.'},\n",
       " {'sent_id': 0, 'tf_score': 0.058823529411764705, 'token': 'is'},\n",
       " {'sent_id': 0, 'tf_score': 0.058823529411764705, 'token': 'my'},\n",
       " {'sent_id': 0, 'tf_score': 0.058823529411764705, 'token': 'favourite'},\n",
       " {'sent_id': 0, 'tf_score': 0.058823529411764705, 'token': 'sport'},\n",
       " {'sent_id': 1, 'tf_score': 0.08333333333333333, 'token': 'my'},\n",
       " {'sent_id': 1, 'tf_score': 0.08333333333333333, 'token': 'favourite'},\n",
       " {'sent_id': 1, 'tf_score': 0.08333333333333333, 'token': 'tv'},\n",
       " {'sent_id': 1, 'tf_score': 0.08333333333333333, 'token': 'serial'},\n",
       " {'sent_id': 1, 'tf_score': 0.08333333333333333, 'token': 'is'},\n",
       " {'sent_id': 1, 'tf_score': 0.08333333333333333, 'token': 'really'},\n",
       " {'sent_id': 1, 'tf_score': 0.08333333333333333, 'token': 'awesome'},\n",
       " {'sent_id': 1, 'tf_score': 0.08333333333333333, 'token': 'and'},\n",
       " {'sent_id': 1, 'tf_score': 0.08333333333333333, 'token': 'i'},\n",
       " {'sent_id': 1, 'tf_score': 0.08333333333333333, 'token': 'enjoy'},\n",
       " {'sent_id': 1, 'tf_score': 0.08333333333333333, 'token': 'watching'},\n",
       " {'sent_id': 1, 'tf_score': 0.08333333333333333, 'token': 'it'},\n",
       " {'sent_id': 2, 'tf_score': 0.1, 'token': 'i'},\n",
       " {'sent_id': 2, 'tf_score': 0.1, 'token': 'dont'},\n",
       " {'sent_id': 2, 'tf_score': 0.1, 'token': 'really'},\n",
       " {'sent_id': 2, 'tf_score': 0.1, 'token': 'want'},\n",
       " {'sent_id': 2, 'tf_score': 0.1, 'token': 'to'},\n",
       " {'sent_id': 2, 'tf_score': 0.1, 'token': 'talk'},\n",
       " {'sent_id': 2, 'tf_score': 0.1, 'token': 'about'},\n",
       " {'sent_id': 2, 'tf_score': 0.1, 'token': 'my'},\n",
       " {'sent_id': 2, 'tf_score': 0.1, 'token': 'favourite'},\n",
       " {'sent_id': 2, 'tf_score': 0.1, 'token': 'colour'},\n",
       " {'sent_id': 3, 'tf_score': 0.06666666666666667, 'token': 'my'},\n",
       " {'sent_id': 3, 'tf_score': 0.06666666666666667, 'token': 'favourite'},\n",
       " {'sent_id': 3, 'tf_score': 0.06666666666666667, 'token': 'subject'},\n",
       " {'sent_id': 3, 'tf_score': 0.06666666666666667, 'token': 'in'},\n",
       " {'sent_id': 3, 'tf_score': 0.06666666666666667, 'token': 'school'},\n",
       " {'sent_id': 3, 'tf_score': 0.13333333333333333, 'token': 'is'},\n",
       " {'sent_id': 3, 'tf_score': 0.13333333333333333, 'token': 'computer'},\n",
       " {'sent_id': 3, 'tf_score': 0.13333333333333333, 'token': 'science'},\n",
       " {'sent_id': 3, 'tf_score': 0.06666666666666667, 'token': ','},\n",
       " {'sent_id': 3, 'tf_score': 0.06666666666666667, 'token': 'because'},\n",
       " {'sent_id': 3, 'tf_score': 0.06666666666666667, 'token': 'awesome'},\n",
       " {'sent_id': 3, 'tf_score': 0.06666666666666667, 'token': '.'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_tf(documents_freq_dicts):\n",
    "    tf_scores = []\n",
    "    for data_collector in documents_freq_dicts:\n",
    "        sent_id = data_collector['sent_id']\n",
    "        for token in data_collector['freq_dict']:\n",
    "            tf_dict = {\n",
    "                'sent_id': sent_id,\n",
    "                'tf_score': data_collector['freq_dict'][token] / data_collector['sent_length'],\n",
    "                'token': token\n",
    "                      }\n",
    "            tf_scores.append(tf_dict)\n",
    "    return tf_scores\n",
    "\n",
    "get_tf(get_document_freq_dict(docs))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sent_id': 0, 'idf_score': 0.28768207245178085, 'token': 'i'},\n",
       " {'sent_id': 0, 'idf_score': 0.28768207245178085, 'token': 'really'},\n",
       " {'sent_id': 0, 'idf_score': 1.3862943611198906, 'token': 'like'},\n",
       " {'sent_id': 0, 'idf_score': 0.6931471805599453, 'token': 'to'},\n",
       " {'sent_id': 0, 'idf_score': 1.3862943611198906, 'token': 'play'},\n",
       " {'sent_id': 0, 'idf_score': 1.3862943611198906, 'token': 'football'},\n",
       " {'sent_id': 0, 'idf_score': 0.6931471805599453, 'token': 'and'},\n",
       " {'sent_id': 0, 'idf_score': 1.3862943611198906, 'token': 'watch'},\n",
       " {'sent_id': 0, 'idf_score': 1.3862943611198906, 'token': 'on'},\n",
       " {'sent_id': 0, 'idf_score': 0.6931471805599453, 'token': 'tv'},\n",
       " {'sent_id': 0, 'idf_score': 0.6931471805599453, 'token': '.'},\n",
       " {'sent_id': 0, 'idf_score': 0.28768207245178085, 'token': 'is'},\n",
       " {'sent_id': 0, 'idf_score': 0.0, 'token': 'my'},\n",
       " {'sent_id': 0, 'idf_score': 0.0, 'token': 'favourite'},\n",
       " {'sent_id': 0, 'idf_score': 1.3862943611198906, 'token': 'sport'},\n",
       " {'sent_id': 1, 'idf_score': 0.0, 'token': 'my'},\n",
       " {'sent_id': 1, 'idf_score': 0.0, 'token': 'favourite'},\n",
       " {'sent_id': 1, 'idf_score': 0.6931471805599453, 'token': 'tv'},\n",
       " {'sent_id': 1, 'idf_score': 1.3862943611198906, 'token': 'serial'},\n",
       " {'sent_id': 1, 'idf_score': 0.28768207245178085, 'token': 'is'},\n",
       " {'sent_id': 1, 'idf_score': 0.28768207245178085, 'token': 'really'},\n",
       " {'sent_id': 1, 'idf_score': 0.6931471805599453, 'token': 'awesome'},\n",
       " {'sent_id': 1, 'idf_score': 0.6931471805599453, 'token': 'and'},\n",
       " {'sent_id': 1, 'idf_score': 0.28768207245178085, 'token': 'i'},\n",
       " {'sent_id': 1, 'idf_score': 1.3862943611198906, 'token': 'enjoy'},\n",
       " {'sent_id': 1, 'idf_score': 1.3862943611198906, 'token': 'watching'},\n",
       " {'sent_id': 1, 'idf_score': 1.3862943611198906, 'token': 'it'},\n",
       " {'sent_id': 2, 'idf_score': 0.28768207245178085, 'token': 'i'},\n",
       " {'sent_id': 2, 'idf_score': 1.3862943611198906, 'token': 'dont'},\n",
       " {'sent_id': 2, 'idf_score': 0.28768207245178085, 'token': 'really'},\n",
       " {'sent_id': 2, 'idf_score': 1.3862943611198906, 'token': 'want'},\n",
       " {'sent_id': 2, 'idf_score': 0.6931471805599453, 'token': 'to'},\n",
       " {'sent_id': 2, 'idf_score': 1.3862943611198906, 'token': 'talk'},\n",
       " {'sent_id': 2, 'idf_score': 1.3862943611198906, 'token': 'about'},\n",
       " {'sent_id': 2, 'idf_score': 0.0, 'token': 'my'},\n",
       " {'sent_id': 2, 'idf_score': 0.0, 'token': 'favourite'},\n",
       " {'sent_id': 2, 'idf_score': 1.3862943611198906, 'token': 'colour'},\n",
       " {'sent_id': 3, 'idf_score': 0.0, 'token': 'my'},\n",
       " {'sent_id': 3, 'idf_score': 0.0, 'token': 'favourite'},\n",
       " {'sent_id': 3, 'idf_score': 1.3862943611198906, 'token': 'subject'},\n",
       " {'sent_id': 3, 'idf_score': 1.3862943611198906, 'token': 'in'},\n",
       " {'sent_id': 3, 'idf_score': 1.3862943611198906, 'token': 'school'},\n",
       " {'sent_id': 3, 'idf_score': 0.28768207245178085, 'token': 'is'},\n",
       " {'sent_id': 3, 'idf_score': 1.3862943611198906, 'token': 'computer'},\n",
       " {'sent_id': 3, 'idf_score': 1.3862943611198906, 'token': 'science'},\n",
       " {'sent_id': 3, 'idf_score': 1.3862943611198906, 'token': ','},\n",
       " {'sent_id': 3, 'idf_score': 1.3862943611198906, 'token': 'because'},\n",
       " {'sent_id': 3, 'idf_score': 0.6931471805599453, 'token': 'awesome'},\n",
       " {'sent_id': 3, 'idf_score': 0.6931471805599453, 'token': '.'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_idf(documents_freq_dict):\n",
    "    idf_scores = []\n",
    "\n",
    "    for data_collector in documents_freq_dict:\n",
    "        #print(data_collector['freq_dict'])\n",
    "        for token in data_collector['freq_dict']:\n",
    "            # Number of documents which contains the specific token\n",
    "\n",
    "            token_freq_docs = sum([\n",
    "                token in data_collector['freq_dict'] \n",
    "                for data_collector in documents_freq_dict\n",
    "            ])\n",
    "            \n",
    "            idf_dict = {\n",
    "                'sent_id': data_collector['sent_id'],\n",
    "                'idf_score': math.log(len(documents_freq_dict) / token_freq_docs),\n",
    "                'token': token\n",
    "                \n",
    "            }\n",
    "            \n",
    "            idf_scores.append(idf_dict)\n",
    "            \n",
    "    return idf_scores\n",
    "get_idf(get_document_freq_dict(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
