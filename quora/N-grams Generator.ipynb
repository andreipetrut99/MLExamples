{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rig',\n",
       " 'igh',\n",
       " 'ght',\n",
       " 'htn',\n",
       " 'tno',\n",
       " 'now',\n",
       " 'oww',\n",
       " 'wwe',\n",
       " 'wew',\n",
       " 'ewa',\n",
       " 'wan',\n",
       " 'ant',\n",
       " 'ntt',\n",
       " 'tto',\n",
       " 'toi',\n",
       " 'oil',\n",
       " 'ilu',\n",
       " 'lus',\n",
       " 'ust',\n",
       " 'str',\n",
       " 'tra',\n",
       " 'rat',\n",
       " 'ate',\n",
       " 'tea',\n",
       " 'ean',\n",
       " 'ang',\n",
       " 'ngr',\n",
       " 'gra',\n",
       " 'ram',\n",
       " 'ams',\n",
       " 'mse',\n",
       " 'sex',\n",
       " 'exa',\n",
       " 'xam',\n",
       " 'amp',\n",
       " 'mpl',\n",
       " 'ple']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_ngrams(text, n, analyzer='words'):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text) # Get rid of special chars\n",
    "    \n",
    "    if analyzer == 'words':\n",
    "        tokenized_text = word_tokenize(text)\n",
    "        print([tokenized_text[start_index:] for start_index in range(n)])\n",
    "        ngrams = zip(*[tokenized_text[start_index:] for start_index in range(n)])\n",
    "        return list(ngrams)\n",
    "    \n",
    "\n",
    "    chars = [\n",
    "        char\n",
    "        for char in text \n",
    "        if char != ' '\n",
    "    ]\n",
    "    ngrams = zip(*[chars[start_index:] for start_index in range(n)])\n",
    "    ngrams = [''.join(tuple_ng) for tuple_ng in ngrams]\n",
    "\n",
    "    return ngrams\n",
    "    \n",
    "text = 'Right now, we want to ilustrate a n-grams example'\n",
    "generate_ngrams(text, 3, 'chars')\n",
    "#generate_ngrams(text, 3, 'words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Right', 'now'), ('now', ','), (',', 'we'), ('we', 'want'), ('want', 'to'), ('to', 'ilustrate'), ('ilustrate', 'a'), ('a', 'n-grams'), ('n-grams', 'example')]\n",
      "[('R', 'i'), ('i', 'g'), ('g', 'h'), ('h', 't'), ('t', ' '), (' ', 'n'), ('n', 'o'), ('o', 'w'), ('w', ','), (',', ' '), (' ', 'w'), ('w', 'e'), ('e', ' '), (' ', 'w'), ('w', 'a'), ('a', 'n'), ('n', 't'), ('t', ' '), (' ', 't'), ('t', 'o'), ('o', ' '), (' ', 'i'), ('i', 'l'), ('l', 'u'), ('u', 's'), ('s', 't'), ('t', 'r'), ('r', 'a'), ('a', 't'), ('t', 'e'), ('e', ' '), (' ', 'a'), ('a', ' '), (' ', 'n'), ('n', '-'), ('-', 'g'), ('g', 'r'), ('r', 'a'), ('a', 'm'), ('m', 's'), ('s', ' '), (' ', 'e'), ('e', 'x'), ('x', 'a'), ('a', 'm'), ('m', 'p'), ('p', 'l'), ('l', 'e')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "\n",
    "# # Word ngrams\n",
    "tokens = word_tokenize(text)\n",
    "word_ngrams = ngrams(tokens, 2)\n",
    "print(list(word_ngrams))\n",
    "\n",
    "# Char ngrams\n",
    "chars = [char for char in text]\n",
    "char_ngrams = ngrams(chars, 2)\n",
    "print(list(char_ngrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
